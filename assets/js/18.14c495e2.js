(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{398:function(t,s,a){t.exports=a.p+"assets/img/mysql-1.f60bd87e.png"},399:function(t,s,a){t.exports=a.p+"assets/img/mysql-2.0dd3cb81.png"},474:function(t,s,a){"use strict";a.r(s);var e=a(42),r=Object(e.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"背景"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#背景"}},[t._v("#")]),t._v(" 背景")]),t._v(" "),e("p",[t._v("我们在开发的过程中使用分页是不可避免的，通常情况下我们的做法是使用limit加偏移量：\n"),e("code",[t._v("select * from table where column=xxx order by xxx limit 1,20")]),t._v("。\n当数据量比较小时（100万以内），无论你翻到哪一页，性能都是很快的。如果查询慢，只要在\nwhere条件和order by 的列上加上索引就可以解决。但是，当数据量大的时候（小编遇到的情况\n是500万数据），如果翻到最后几页，即使加了索引，查询也是非常慢的，这是什么原因导致的呢？我们该如何解决呢？")]),t._v(" "),e("h2",{attrs:{id:"limit分页原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#limit分页原理"}},[t._v("#")]),t._v(" limit分页原理")]),t._v(" "),e("p",[t._v("当我们翻到最后几页时，查询的sql通常是："),e("code",[t._v("select * from table where column=xxx order by xxx limit 1000000,20")]),t._v("。\n查询非常慢。但是我们查看前几页的时候，速度并不慢。这是因为limit的偏移量太大导致的。\nMySql使用limit时的原理是（用上面的例子举例）：")]),t._v(" "),e("ol",[e("li",[t._v("MySql将查询出1000020条记录。")]),t._v(" "),e("li",[t._v("然后舍掉前面的1000000条记录。")]),t._v(" "),e("li",[t._v("返回剩下的20条记录。")])]),t._v(" "),e("p",[t._v("上述的过程是在《高性能MySql》书中确认的。")]),t._v(" "),e("h2",{attrs:{id:"解决方案"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[t._v("#")]),t._v(" 解决方案")]),t._v(" "),e("p",[t._v("解决的方法就是尽量"),e("strong",[t._v("使用索引覆盖扫描")]),t._v("，就是我们select后面检出的是索引列，而不是\n所有的列，而且这个索引的列最好是id。然后再做一次关联查询返回所有的列。\n上述的sql可以写成：")]),t._v(" "),e("div",{staticClass:"language-sql extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sql"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v("\n\t"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v("\n\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" t\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INNER")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v("\n\t\tid\n\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v("\n\t\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v("\n\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v("\n\t\txxx_id "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("143381")]),t._v("\n\t"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("LIMIT")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("800000")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t1 "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" t"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t1"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id\n")])])]),e("p",[t._v("我们在mysql中做的真实的实验：")]),t._v(" "),e("p",[e("img",{attrs:{src:a(398),alt:"image1"}})]),t._v(" "),e("p",[t._v("上图是没有优化过的sql，执行时间为2s多。经过优化后如下：")]),t._v(" "),e("p",[e("img",{attrs:{src:a(399),alt:"image2"}})]),t._v(" "),e("p",[t._v("执行时间为0.3s，性能有了大幅度的提升。虽然做了优化，但是随着偏移量的增加，性能也会随着下降，MySql官方虽然也给出了\n其他的解决方案，但是在实际开发中很难使用。")]),t._v(" "),e("p",[e("strong",[t._v("有的同学可能会问，能不能使用"),e("code",[t._v("IN")]),t._v("嵌套子查询，而不使用"),e("code",[t._v("INNER JOIN")]),t._v("的方式，答案是不可以，因为MySql在子查询中不能使用"),e("code",[t._v("LIMIT")]),t._v("。")])]),t._v(" "),e("p",[t._v("MySql分页优化就先介绍到这里了。")])])}),[],!1,null,null,null);s.default=r.exports}}]);